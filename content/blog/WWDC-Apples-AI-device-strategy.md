---
title: WWDC and Apple's AI device strategy
description: "Liquid Glass is a clear (ha ha) hint at Apple's AI strategy."
date: 2025-06-18
tags:
  - AI
  - Apple
  - design
---

Right after WWDC, I was feeling underwhelmed and a little baffled. But with a week of hindsight, I think the commentary was so focused on the aesthetics of Liquid Glass, it missed the function. Liquid Glass tells me that Apple thinks your AI device of the future isn’t a new wearable, but the iPhone you already own.

Google, OpenAI, and Meta have world-class LLMs, but an LLM isn’t a product until it’s in something you can use. Google and Meta are making glasses. Jony Ive is making a brushed-aluminium…something. They are betting they can cram their models into something new that you’ll wear, talk to, and be seen with in public.

Apple's current on-device LLM sucks. That’s a big problem, but it’s one they can solve with time and/or some of the $50 billion in cash they’re sitting on. More importantly, it’s the easy part of the AI device problem to solve. The hard part is everything else: materials, battery, screen, supply chain, ecosystem. The stuff the iPhone has already solved.

If the iPhone is your AI device, it’s going to need a new interface. That's where Liquid Glass comes in. It's not aesthetically my cup of tea, but everything about Liquid Glass is designed to merge digital UI and physical context:
- You can literally see through it. (Welcome back, Frutiger Aero!)
- It adapts blur and brightness dynamically over depth-of-field images
- Menus unfold in place instead of breaking flow with submenus
- Modal-less interfaces get more emphasis and power, letting you take actions without breaking context
- Camera, once a tangle of indistinguishable "modes," launches with two buttons
- Spotlight is a lightweight assistant, not a dumb search bar
 
This is how you prepare the iPhone to be a personal AI device: by getting the UI out of the way. Apple thinks you'll spend a lot more time simultaneously looking at and through your phone. They want the screen to be a window, not a camera. To me, this is a much more plausible theory than getting people used to glasses or a pin or a glowing pebble on a necklace and solving every attendant hardware and interface challenge from scratch.

This may work or it may not. Tim Cook needs to lace up his Nikes and get into the LLM race. But the "Apple is washed up" vibe shift has gone way too far. They already own the hardest part of an AI assistant to get right: the device. The rest is about money and design sense. Magic Mouse charging port aside, Apple's got a lot of both. Who are you betting on?

(h/t to Tara Tan, whose [provocative recent post on this](https://www.linkedin.com/posts/tantara_today-design-royalty-has-been-dethroned-activity-7337927311045926912-BqFB) inspired me to get my own thoughts down on pixel.)

*This post was originally [published on LinkedIn](https://www.linkedin.com/posts/danmunz_wwdc-activity-7340717363371143170--Z8o/). Since then, Apple has indeed [started exploring swapping out its own LLM with a competitor's](https://archive.is/BXtd4).*
  
