<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Dan Munz</title>
  <subtitle>Thoughts on digital government, democracy, tech, public institutions, and the occasional unrelated rabbit hole.</subtitle>
  <link href="https://danmu.nz/feed/feed.xml" rel="self" />
  <link href="https://danmu.nz/" />
  <updated>2025-08-06T00:00:00Z</updated>
  <id>https://danmu.nz/</id>
  <author>
    <name>Dan Munz</name>
  </author>
  <entry>
    <title>The end of civic tech&#39;s interface era</title>
    <link href="https://danmu.nz/blog/the-end-of-civic-techs-interface-era/" />
    <updated>2025-08-06T00:00:00Z</updated>
    <id>https://danmu.nz/blog/the-end-of-civic-techs-interface-era/</id>
    <content type="html">&lt;p&gt;&lt;span class=&quot;note&quot;&gt;This post was co-authored with Mark Headd, my colleague at Ad Hoc and a brilliant thinker and doer on technology and government. It&#39;s less a prediction than a provocation, playing out one possible consequence of generative AI to its logical endpoint. Check out &lt;a href=&quot;https://civic.io/&quot;&gt;Mark&#39;s blog&lt;/a&gt; and &lt;a href=&quot;https://www.linkedin.com/in/markheadd/&quot;&gt;follow him on LinkedIn&lt;/a&gt; for more of his great writing.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There are lots of ways to answer the question &amp;quot;what does a civic technologist do,&amp;quot; but a decent umbrella description is: work to bridge the gap between people and government by designing and building better digital interfaces. We organize information, build user-focused digital solutions, streamline forms, and redesign websites. This will, we hope, make government services more accessible, efficient, and user-friendly.&lt;/p&gt;
&lt;p&gt;This work has operated under an assumed premise so fundamental that it&#39;s rarely made explicit: That the digital interfaces we build when people interact with governments are themselves highly valuable. From this, it follows that the value of our work is embedded and reflected in the design and technical proficiency with which these interfaces are implemented.&lt;/p&gt;
&lt;p&gt;It&#39;s not surprising that the civic tech world has largely metabolized the rise of Artificial Intelligence (AI) as a set of tools we can use to make these interfaces &lt;em&gt;even better&lt;/em&gt;. Chatbots! &lt;a href=&quot;https://codeforamerica.org/news/our-ai-solution-to-governments-pdf-problem/&quot;&gt;Accessible PDFs!&lt;/a&gt; These are good and righteous efforts that make things easier for government employees and better for the people they serve. But they&#39;re sitting on a fault line that AI is shifting beneath our feet: What if the primacy and focus we give *interfaces, *and the constraints we&#39;ve accepted as immutable, are changing?&lt;/p&gt;
&lt;h3 id=&quot;the-old-paradigm-the-interface-is-all&quot;&gt;The old paradigm: the interface is all&lt;/h3&gt;
&lt;p&gt;&amp;quot;Humans interacting with graphical user interfaces through the browser&amp;quot; - that&#39;s what the web…&lt;em&gt;is&lt;/em&gt;. Right? So why &lt;em&gt;wouldn&#39;t&lt;/em&gt; we consider the interface to be the definitive digital expression of a federal agency to its customers? But if you shift this from a rhetorical question to an actual one, you find that this paradigm rests on three key assumptions:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fixed, durable designs&lt;/strong&gt;: Once launched, an interface becomes relatively static. It is (hopefully) improved incrementally based on user feedback and changing needs. About once a decade, some combination of new political appointees and end-of-life technologies will cause someone to utter the dreaded &lt;em&gt;redesign&lt;/em&gt; word. Barring that, frequent, large-scale changes to an agency&#39;s website are expensive, time-consuming, and risky because a single change affects every user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Multitenant by necessity&lt;/strong&gt;: Everyone uses essentially the same interface. There are limited variations for different devices, languages, or accessibility needs. Mobile users and desktop users will typically see different manifestations of the same interface. But the &lt;a href=&quot;https://developer.mozilla.org/en-US/docs/Learn_web_development/Core/CSS_layout/Responsive_Design&quot;&gt;same scaffolding&lt;/a&gt; is typically used to serve these different user groups.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;High expertise requirements&lt;/strong&gt;: Because we have just one basic scaffolding to work with, we take a utilitarian view, using techniques like personas, card sorts, web analytics, and top tasks to approximate and serve the median user. Techniques require technicians: Building quality government websites has traditionally required teams of UX designers, developers, content strategists, and accessibility experts working together to produce the interface that the user ultimately sees. Because these interfaces represent business processes often encoded in law or regulation, these teams also expend effort designing and constructing the optimal user journey.&lt;/p&gt;
&lt;h3 id=&quot;enter-just-in-time-interfaces&quot;&gt;Enter just-in-time interfaces&lt;/h3&gt;
&lt;p&gt;AI upsets each one of these old assumptions both &lt;a href=&quot;https://signalpath.substack.com/p/just-in-time-interfaces&quot;&gt;conceptually and practically&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Just-in-Time UI is an emerging design paradigm that shifts the burden of navigation away from the user, and instead delivers the right interface at the right moment, in the right context. Where traditional interfaces rely on static screens, fixed menus, and user-initiated flows, Just-in-Time UI reimagines the interaction model.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Modern generative AI tools can assemble complex, high-fidelity interfaces quickly and cheaply. If you&#39;re a civic designer used to hand-crafting bespoke interfaces with care, the idea of just-in-time interfaces in production makes your hair stand on end. Us, too. The reality is, this is still an idea that lies in the future. But the future is getting here very quickly.&lt;/p&gt;
&lt;p&gt;Shopify, with its &lt;a href=&quot;https://www.demandsage.com/shopify-statistics/&quot;&gt;5M DAUs and $292B processed annually&lt;/a&gt;, is &lt;a href=&quot;https://x.com/postcarl/status/1942616904952340827&quot;&gt;doing its internal prototyping&lt;/a&gt; with generative AI. Delivering production UIs this way is gaining steam both &lt;a href=&quot;https://isolutions.medium.com/ephemeral-ui-in-ai-generated-on-demand-interfaces-81dbc8cd4579&quot;&gt;in theory&lt;/a&gt; and in proof-of-concept (e.g., &lt;a href=&quot;https://arxiv.org/abs/2405.09255#&quot;&gt;adaptive UIs&lt;/a&gt;, Fred Hohman&#39;s &lt;a href=&quot;https://fredhohman.com/papers/biscuit&quot;&gt;Project Biscuit&lt;/a&gt;, Sean Grove&#39;s &lt;a href=&quot;https://www.youtube.com/watch?v=xgi1YX6HQBw&quot;&gt;ConjureUI demo&lt;/a&gt;). The idea is serious enough that Google, not a slouch in the setting-web-standards game, is getting into the mix with &lt;a href=&quot;https://developers.googleblog.com/en/stitch-a-new-way-to-design-uis/&quot;&gt;Stitch&lt;/a&gt; and &lt;a href=&quot;https://developers.googleblog.com/en/introducing-opal/&quot;&gt;Opal&lt;/a&gt;. AWS is &lt;a href=&quot;https://partyrock.aws/&quot;&gt;throwing its hat in the ring too&lt;/a&gt;. Smaller players like &lt;a href=&quot;https://www.buildai.space/&quot;&gt;BuildAI&lt;/a&gt;, &lt;a href=&quot;https://replit.com/ai&quot;&gt;Replit&lt;/a&gt;, &lt;a href=&quot;https://www.figma.com/ai/&quot;&gt;Figma&lt;/a&gt;, and &lt;a href=&quot;https://camunda.com/&quot;&gt;Camunda&lt;/a&gt; are exploring LLM-driven UI generation and workflow design. All of these &lt;em&gt;at first&lt;/em&gt; may generate wacky interfaces and &lt;a href=&quot;https://archive.is/ExLk7&quot;&gt;internet horror stories&lt;/a&gt;, and right now they&#39;re mostly focused on dynamic UI generation for a &lt;em&gt;developer&lt;/em&gt;, not a &lt;em&gt;user&lt;/em&gt;. But these are all different implementations of an idea that are converging on a clear endpoint, and if they can get into use at any substantial scale, they will become more reliable and production ready very quickly.&lt;/p&gt;
&lt;p&gt;Civic designers shouldn&#39;t fear this future; we&#39;ve been preparing for it. The Shopify tool mentioned above works in part because their &lt;a href=&quot;https://medium.com/eightshapes-llc/tokens-in-design-systems-25dd82d58421&quot;&gt;design system is built on tokens&lt;/a&gt;, named variables that store key aspects of a design system. Tokens aren&#39;t new and aren&#39;t rocket science, but they are a best practice. You know which other design systems are tokenized? The &lt;a href=&quot;https://designsystem.digital.gov/design-tokens/&quot;&gt;U.S. Web Design System&lt;/a&gt;, the &lt;a href=&quot;https://design.va.gov/foundation/design-tokens&quot;&gt;VA.gov Design System&lt;/a&gt;, the &lt;a href=&quot;https://designsystem.cancer.gov/foundations&quot;&gt;National Cancer Institute Design System&lt;/a&gt;, the &lt;a href=&quot;https://dcs.colorado.gov/ids/digital-guidelines/design-tokens&quot;&gt;State of Colorado Design System&lt;/a&gt;, and many, many others. Tokens aren&#39;t &lt;em&gt;sufficient&lt;/em&gt; to make just-in-time UIs a reality, but they probably are &lt;em&gt;foundational&lt;/em&gt;, and thanks to USWDS and its progeny, they are near ubiquitous in dot-gov domains.&lt;/p&gt;
&lt;p&gt;The point is this: civic designers have both a strong foundation and a growing imperative to think seriously about what government websites will look like in a world of just-in-time UI generation.&lt;/p&gt;
&lt;h3 id=&quot;humans-centered-design&quot;&gt;Humans-centered design&lt;/h3&gt;
&lt;p&gt;It&#39;s often said that &amp;quot;creativity loves constraints,&amp;quot; and today&#39;s dot-gov ecosystem reflects the constraints of its builders. Civic designers build government websites for the median user and try to offer top-tasks and no-wrong-doors because, for the most part, we have to pick one basic interface for everyone. But if just-in-time UIs erodes this constraint, we can see the edges of a new approach to delivering improved government services via the web:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adaptive, Context-Specific Interactions&lt;/strong&gt;: Instead of designing one interface that works acceptably for everyone, and must be managed and scaled that way, AI will make it possible to generate variations tailored to specific circumstances. A single parent applying for childcare assistance in their state might see a completely different interface than a recent college graduate seeking benefits under the same program–a different information architecture, different interaction patterns, and different support options. A user whose natural language input indicates they&#39;re eligible for two different benefits programs might be served a dynamically-generated form that applies for both simultaneously. Deciding what to present when will be a matter not of documenting discrete personas and user journeys, but of training and guiding a generative AI model. While core navigation and other elements can remain consistent, the specific guidance, information hierarchy, and interactive elements can be tailored to each user&#39;s specific situation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Expertise Moves Upstream&lt;/strong&gt;: The expertise needed to build high-quality digital services will move upstream–from implementation to architecture, from specific solutions to systemic standards. Civic tech experts will focus on creating and refining the building blocks, standards, and business logic that AI systems use to tailor interfaces on demand. UX designers will shift from crafting individual screens to defining interaction principles and component libraries, and from hand-coding each UI element to deciding which elements should be dynamically generated or held constant over time. Content strategists will develop frameworks for how information should be structured and presented across different contexts. This kind of UX infrastructure, today often thought of as &amp;quot;bonus features&amp;quot; of a web modernization effort, will instead become &lt;em&gt;foundational&lt;/em&gt;, comprising a kind of &lt;a href=&quot;https://en.wikipedia.org/wiki/AI_alignment&quot;&gt;alignment&lt;/a&gt; that is focused on maintaining visual identity and interaction guardrails.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defining Policy as Code&lt;/strong&gt;: Design is more than how a website looks, and just-in-time interfaces will need more than a prompt and a design system to generate a coherent web workflow for filing taxes or applying for public benefits. To bridge the gap between &lt;em&gt;prototyping or coding&lt;/em&gt; with AI and &lt;em&gt;generating just-in-time interfaces in production&lt;/em&gt;, we also have to invest in &lt;a href=&quot;https://digitalgovernmenthub.org/get-involved/digital-benefits-network-rules-as-code-community-of-practice/&quot;&gt;encoding the business logic&lt;/a&gt; that users have to traverse in ways that generative AI systems can understand. Tools and formats like &lt;a href=&quot;https://bpmn.io/&quot;&gt;BPMN/DMN&lt;/a&gt;, &lt;a href=&quot;https://www.openpolicyagent.org/docs/policy-language&quot;&gt;Rego&lt;/a&gt;, and &lt;a href=&quot;https://catala-lang.org/en/examples/us-tax-code&quot;&gt;Catala&lt;/a&gt; offer models for encoding policy, and generative AI can &lt;a href=&quot;https://adhoc.team/2024/09/25/ai-policy-to-code/&quot;&gt;build pipelines for translation&lt;/a&gt;. As with design systems, the just-in-time UI world will see civic researchers and designers focus less on how a particular form looks, and more on specifying the visual, policy, and process ingredients that generative AI can combine to meet a user&#39;s needs.&lt;/p&gt;
&lt;h3 id=&quot;the-opportunity-ahead&quot;&gt;The opportunity ahead&lt;/h3&gt;
&lt;p&gt;We are at an inflection point in civic technology. The same AI capabilities transforming other sectors will revolutionize how people interact with their government. Realizing this potential requires rethinking our role as civic technologists. Instead of building fixed solutions, we need to become architects of adaptive systems. Instead of designing for the average user, we need to create frameworks that can be adapted into interfaces that meet a person&#39;s unique circumstances.&lt;/p&gt;
&lt;p&gt;While these are radically new &lt;em&gt;ways&lt;/em&gt; of designing experiences, the &lt;em&gt;purpose&lt;/em&gt; of designing them is the same as it always was: Putting the user at the center of the interaction. Just-in-time interfaces aren&#39;t inevitable; to adopt them, public sector agencies will have to have a high tolerance for disrupting team structures and embracing risk. But if implemented responsibly, the potential payoff is staggering: We can design interfaces for &lt;em&gt;actual users&lt;/em&gt;, not lossy simulacra defined by personas and bounce rates. We can flex experiences dynamically to meet individual users&#39; reading levels, socioeconomic context, and most urgent priorities.&lt;/p&gt;
&lt;p&gt;To do this, we need to change the way we think about the interfaces that people use to transact and interact with government agencies. We have to design from the start with the intention to move from fixed, costly, multitenant experiences to adaptive, tailored, highly individualized ones.&lt;/p&gt;
&lt;p&gt;More than a decade ago, when the civic tech movement reorganized itself after its fledgling early years around the principles of a user&#39;s experience with online government services, we put the interfaces we build at the center of the civic tech universe. If we&#39;re going to realize the benefits that AI can bring to the way that people interact with their government, we need to change our way of thinking about what interfaces are, how people use them, and most importantly, how we build them.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>AI thinking, fast and slow</title>
    <link href="https://danmu.nz/blog/ai-thinking-fast-and-slow/" />
    <updated>2025-08-02T00:00:00Z</updated>
    <id>https://danmu.nz/blog/ai-thinking-fast-and-slow/</id>
    <content type="html">&lt;p&gt;I&#39;m increasingly intrigued by a concept called Hierarchical Reasoning Models (HRM), a potential alternative architecture to traditional LLMs.&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/ai-thinking-fast-and-slow/#traditional-llms-note&quot; id=&quot;traditional-llms-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; Last month, Sapient released an &lt;a href=&quot;https://github.com/sapientinc/HRM&quot;&gt;open-source HRM&lt;/a&gt; and accompanying &lt;a href=&quot;https://arxiv.org/abs/2506.21734&quot;&gt;paper&lt;/a&gt;. The paper is written in the kind of prose you&#39;d expect from nine mathematics PhDs&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/ai-thinking-fast-and-slow/#math-phd-prose-note&quot; id=&quot;math-phd-prose-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt;, but as far as I can tell, the basic idea is this:&lt;/p&gt;
&lt;p&gt;LLMs &amp;quot;reason&amp;quot; one word or concept at a time. This causes a limitation that the HRM paper authors call &amp;quot;brittle task decomposition,&amp;quot; an academic way of saying that a flaw in one link in the chain of reasoning can derail the whole thing.&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/ai-thinking-fast-and-slow/#icebreaker-linear-thinking-note&quot; id=&quot;icebreaker-linear-thinking-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; You can debate how big a problem this is; even general-purpose LLMs are incredibly capable tools. But HRMs, at least in theory, reason more like we do:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide, and fast, lower-level circuits to execute—subordinate processing while preserving global coherence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, HRM outputs are governed by distinct processes working at different speeds, with &amp;quot;slower,&amp;quot; more deliberative background processes governing faster, more impulsive ones. This separation of cognitive concerns will be familiar to readers of Kahneman&#39;s &lt;em&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow&quot;&gt;Thinking, Fast and Slow&lt;/a&gt;&lt;/em&gt;, which the authors explicitly credit as inspiration:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The brain dynamically alternates between automatic thinking...and deliberate reasoning. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network...Inspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables “thinking, fast and slow”.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Again, a smarter person than me might read this paper/model and call it bunk, but assuming it&#39;s conceptually sound, there are a few reasons I think this is exciting especially for public sector applications:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;They&#39;re open-source (at least, this one is).&lt;/strong&gt; This isn&#39;t a &lt;em&gt;feature&lt;/em&gt; of HRMs and there are &lt;a href=&quot;https://github.com/eugeneyan/open-llms&quot;&gt;plenty of open-source LLMs&lt;/a&gt;, but it&#39;s a good thing that HRM development is starting in earnest in a transparent, auditable way.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;They break the size-quality paradigm.&lt;/strong&gt;  HRMs are less compute- and cost-intensive. This reduces the &amp;quot;success penalty&amp;quot; for AI adoption, and presents a model of improving LLM performance that isn&#39;t just &lt;code&gt;MOAR TOKENZ!!1!&lt;/code&gt;. Emerging evidence (and common sense) suggests that ginormous context windows increase cost without increasing–and in some cases, degrading–quality. HRMs are a nice reminder to government buyers to demand smarter architectures for their taxpayer dollar, not just &lt;em&gt;bigness&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Public services have context.&lt;/strong&gt; I&#39;d like to think we&#39;ll only ever use AI to eliminate toil, but it&#39;s inevitable that it will be put in-the-loop on benefits adjudication, financial aid decisions, FDA approvals, etc. On paper these are clear, stepwise processes; in reality, they take place against a background of novel individual circumstances, regulatory and legal frameworks, and small-p political initiatives. Can a linear LLM, even with an infinite context window, consider factors like these in a timely and non-budget-exploding way? HRMs&#39; ability to run latent reasoning and revisit prior steps makes it at least &lt;em&gt;plausible&lt;/em&gt; that with the right rule-based inputs they could more reliably &amp;quot;sanity-check&amp;quot; outcomes against the intent of laws, regulations, and policy initiatives, and prioritize fairness and precedent alongside speed and compliance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Less random lying!&lt;/strong&gt; LLMs work by guessing at what someone who just said the &lt;em&gt;last&lt;/em&gt; thing it said would say next. We have a word for humans who think this way: sociopaths. LLMs reason themselves into corners they have to lie to get out of. Sometimes this manifests as ChatGPT asking if you&#39;d like a Word document, apparently &lt;a href=&quot;https://www.reddit.com/r/ChatGPT/comments/1jrsb0e/chatgpt_offering_to_create_word_document_lying_it/&quot;&gt;forgetting that it doesn&#39;t have the ability to make Word documents&lt;/a&gt;. Other times, it &lt;a href=&quot;https://amandaguinzburg.substack.com/p/diabolus-ex-machina&quot;&gt;offers edits on content it hasn&#39;t read&lt;/a&gt;. Or &lt;a href=&quot;https://www.reddit.com/r/ChatGPT/comments/1m4lsso/replit_ai_went_rogue_deleted_a_companys_entire/&quot;&gt;deletes an entire production database&lt;/a&gt;:&lt;br&gt;&lt;br&gt;&lt;blockquote class=&quot;reddit-embed-bq&quot; style=&quot;height:500px&quot; data-embed-height=&quot;740&quot;&gt;&lt;a href=&quot;https://www.reddit.com/r/ChatGPT/comments/1m4lsso/replit_ai_went_rogue_deleted_a_companys_entire/&quot;&gt;Replit AI went rogue, deleted a company&#39;s entire database, then hid it and lied about it&lt;/a&gt;&lt;br&gt; by&lt;a href=&quot;https://www.reddit.com/user/MetaKnowing/&quot;&gt;u/MetaKnowing&lt;/a&gt; in&lt;a href=&quot;https://www.reddit.com/r/ChatGPT/&quot;&gt;ChatGPT&lt;/a&gt;&lt;/blockquote&gt;&lt;script async=&quot;&quot; src=&quot;https://embed.reddit.com/widgets.js&quot; charset=&quot;UTF-8&quot;&gt;&lt;/script&gt;&lt;br&gt;Not great! I am admittedly departing into speculation here, but HRMs &lt;em&gt;seem&lt;/em&gt; less likely to engage in this kind of behavior. By design, they can think before they speak and form complete answers, retrace their steps and take more time if needed, and keep what they&#39;ve said in distinct working memory. As far as I know there&#39;s been no head-to-head testing on this, but there&#39;s already evidence that &lt;a href=&quot;https://arxiv.org/abs/2310.06271&quot;&gt;prompting self-reflection reduces hallucinations in LLMs&lt;/a&gt;–HRMs seem to just bake this idea into the architecture.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Federal agencies are &lt;a href=&quot;https://www.gao.gov/products/gao-25-107653&quot;&gt;adopting AI with startling speed&lt;/a&gt;, which seems risky, but maybe less risky than getting left behind. HRMs offer at least a foundation for a more appealing third option, and push us to look for more sophisticated and more efficient architectures, not just more and bigger AI.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>A few thoughts on seeing The Pixies live</title>
    <link href="https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/" />
    <updated>2025-07-28T00:00:00Z</updated>
    <id>https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/</id>
    <content type="html">&lt;p&gt;Because my buddy Todd is rad, I got to see &lt;a href=&quot;https://www.setlist.fm/setlist/pixies/2025/the-anthem-washington-dc-4b51cbb2.html&quot;&gt;The Pixies live at The Anthem in DC this weekend&lt;/a&gt;. It was genuinely one of the best shows I&#39;ve ever been to. A few reflections in no particular order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;They are incredible musicians. Our mental model of The Pixies is distortion-screamy-quiet-twangy but this team knows and loves their instruments and plays with precision and care. Dave Lovering could not have been deeper in the pocket and Joey Santiago played with a depth and complexity that was still shocking even on songs where I could sing every chord. New(ish) bassist Emma Richardson is absolutely sick — more on her later.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;They are &lt;strong&gt;LOUD AS FUCK&lt;/strong&gt;. Something about the way they produce their records kind of hides or flattens this. Maybe it’s just not mechanically possible to &lt;em&gt;record&lt;/em&gt; how loud they actually play and still have it be recognizable as music? In person they are face-meltingly loud. Not just in decibels, but in the &lt;em&gt;density&lt;/em&gt; of sound coming at you per second. It’s almost like a Wall of Sound effect. If music emitted heat instead of sound, the audience would have been microwaved. I’ve seen &amp;quot;harder&amp;quot; or technically louder bands live, but I’ve never felt as &lt;em&gt;physically dissolved by sound waves&lt;/em&gt; at a concert as I did on Saturday.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Seeing them live really brought home to me why they were so many people&#39;s gateway drug into alternative/indie/post-pop/choose-your-label. Their music is rich with ideas, but that richness comes as much from their inputs as their outputs: In a ~90 minute show we heard surf rock, doo-wop, jangly pop, punk, R&amp;amp;B, ballads. Of all the bands of their generation, I think what The Pixies uniquely figured out was how to reject the synths and formalism and artifice that was polluting popular music but not throw away the pop formulas and popular styles that came before it.&lt;br&gt;&lt;br&gt;iTunes tells me they are &amp;quot;alternative&amp;quot; but I spent the concert time happily bopping and losing my shit like a 16-year-old at a Beatles concert. A fan of The Archies could get down with &lt;em&gt;Here Comes Your Man&lt;/em&gt;.  If you like Emerson, Lake &amp;amp; Palmer you’ll love &lt;em&gt;Motorroller&lt;/em&gt;.  &lt;em&gt;Hey&lt;/em&gt; is basically an R&amp;amp;B/funk single. &lt;em&gt;Mr. Grieves&lt;/em&gt; has traces of Hank Williams crackling over the radio, and &lt;em&gt;Chicken&lt;/em&gt; is almost a Chi-Lites song. They were (and are) deeply inventive, but what they invented were ways to revivify &lt;a href=&quot;https://en.wikipedia.org/wiki/Black_Francis#Influences&quot;&gt;the sounds they grew up loving&lt;/a&gt;, just with weird structures and chord changes and time signatures (I think Vamos is in 6/4?) and like 25% faster and 75% weirder, but their tone is easy to listen to. This really comes through live: The guitars sound like guitars, the bass sounds like bass&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/#gouge-away-edm-note&quot; id=&quot;gouge-away-edm-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt;, the drums sure as shit sound like drums. They inspired the Nirvanas and Radioheads and for sure did some weird shit themselves&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/#something-against-amp-note&quot; id=&quot;something-against-amp-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt;–but the &lt;em&gt;sound&lt;/em&gt; of Pixies music is deeply recognizable as human people playing instruments.&lt;br&gt;&lt;br&gt;At a time when the 80s were still trying to choke the world with hairspray and styrofoam, the Pixies showed up with a beach umbrella and a picnic basket full of sandwiches, if sandwiches were for some reason made out of methamphetamines. Hearing them live, you really understand why they’re one of those &amp;quot;everyone who heard their album started a band&amp;quot; bands — they were probably the first and definitely the best of their generation to achieve Hegelian synthesis&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/#hegel-not-really-note&quot; id=&quot;hegel-not-really-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; of what had come before.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pixies songs are &lt;em&gt;compact&lt;/em&gt;. Their lyrics are dark and weird but also funny and not ponderous or discursive. (&lt;em&gt;Gouge Away&lt;/em&gt;, a retelling of Samson&#39;s betrayal by Delilah and ultimate death at the hands of the Philistines, is 105 words long.) At the concert on Saturday, without stopping between songs, they covered 25 songs in about 90 minutes. Out of curiosity I looked it up afterwards: From &lt;em&gt;Come on Pilgrim&lt;/em&gt; to &lt;em&gt;Trompe Le Monde&lt;/em&gt; there are four (4) songs over four minutes. Sonic Youth’s &lt;em&gt;Goo&lt;/em&gt; has seven. But it&#39;s not just brevity–the shape of Pixies songs are surgical and sudden, and you feel it live: Nothing is dragging, ever, even on the slow songs. So that probably didn’t hurt them in terms of getting some mainstream popularity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Emma Richardson. Jesus christ she&#39;s so good. Her playing was bouncy and locked in and her vocals were gorgeous. They ended with &lt;em&gt;Into the White&lt;/em&gt;, and she brought the house down. A little part of me was prepared to miss Kim Deal, but that...did not happen. (I&#39;m sorry, I&#39;m sorry, I&#39;ll go put on &lt;em&gt;Last Splash&lt;/em&gt;.)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;One of the coolest moments of the night: Kurt Vile opened (and was phenomenal). About halfway into the Pixies set, he wanders into our section and grabs a table next to us and acts...like a fan. Taking photos, doing the guitar fingerings for his favorite songs. Even your idols have idols.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Singing the &amp;quot;ooo ooo&amp;quot; part of &lt;em&gt;Where Is My Mind&lt;/em&gt; with 5,000 strangers was genuine magic.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/3crhk1bJhv-1024.avif 1024w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/3crhk1bJhv-1024.webp 1024w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://danmu.nz/blog/a-few-thoughts-on-seeing-the-pixies-live/3crhk1bJhv-1024.jpeg&quot; alt=&quot;&quot; width=&quot;1024&quot; height=&quot;768&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>LLMs can learn things we don&#39;t teach them</title>
    <link href="https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/" />
    <updated>2025-07-23T00:00:00Z</updated>
    <id>https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/</id>
    <content type="html">&lt;p&gt;I had to read &lt;a href=&quot;https://arxiv.org/abs/2507.14805&quot;&gt;this one&lt;/a&gt; twice:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In our main experiments, a &amp;quot;teacher&amp;quot; model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a &amp;quot;student&amp;quot; model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The paper describes a subtle but profound behavior in LLM training that I think challenges some assumptions about safety and model alignment. The actual finding is summed up neatly at the end (emphasis mine):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Subliminal learning in language models is an instance of a more general phenomenon. We prove that &lt;strong&gt;when a student is trained to imitate a teacher that has nearly equivalent parameters, the parameters of the student are pulled toward the parameters of the teacher&lt;/strong&gt;. This, in turn, means that the outputs of the student are pulled toward the outputs of the teacher, &lt;strong&gt;even on inputs that are far from the training distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are some important caveats (mainly, this effect appears only when the student and teacher models share the same base model) but the result is startling: a model can inherit behavioral tendencies from a teacher even when trained on filtered, semantically unrelated data. &lt;em&gt;The mechanism isn&#39;t semantic, it&#39;s statistical and architectural.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I am not enough of a Ph.D. mathematician to know what this means from a deep-in-the-model POV. But at least in modern times, I don&#39;t think any technology has &lt;a href=&quot;https://www.hks.harvard.edu/publications/rapid-adoption-generative-ai&quot;&gt;reached consumer ubiquity&lt;/a&gt; at a speed that so far outpaces our understanding of it. As practitioner of government things, the paper is a bracing reminder: &lt;strong&gt;While we understand how LLMs operate at the architectural level, their higher-level behaviors, especially under distillation and fine-tuning, remain unpredictable in ways that matter for public trust.&lt;/strong&gt; The &lt;em&gt;specific&lt;/em&gt; setup from the Anthropic paper seems unlikely to occur &amp;quot;outside the lab&amp;quot; in government AI today, but as agencies begin fine-tuning foundation models or training smaller models on synthetic data from general-purpose LLMs, the boundary between “lab” and “field” will blur.&lt;/p&gt;
&lt;p&gt;What should civic tech practitioners take away from the Anthropic paper?&lt;/p&gt;
&lt;h3 id=&quot;keep-humans-in-the-loop&quot;&gt;Keep humans in the loop&lt;/h3&gt;
&lt;p&gt;&amp;quot;&lt;a href=&quot;https://x.com/bumblebike/status/832394003492564993&quot;&gt;A computer can never be held accountable, therefore a computer must never make a management decision&lt;/a&gt;&amp;quot; is as true as ever. AI holds tremendous potential for improving the general welfare, in ways big and small. (&lt;a href=&quot;https://www.propel.app/insights/using-ai-to-help-snap-recipients-diagnose-and-restore-lost-benefits/&quot;&gt;Dave Guarino&#39;s work at Propel&lt;/a&gt; is especially compelling on this point.) But to me, the risks of putting AI &lt;em&gt;in the loop&lt;/em&gt; on, say, a benefits adjudication decision remain high.&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/#covid-fonts-note&quot; id=&quot;covid-fonts-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; Subliminal learning adds another layer to that risk: &lt;strong&gt;even if we carefully sanitize training data, a model distilled from a biased or misaligned one could pass down weird decision patterns that elude audit or explanation.&lt;/strong&gt; We have to think not just about what we tell a model explicitly, but what its weights...&lt;em&gt;remember&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Things can still go wrong in ways that are unpredictable, possibly undetectable except at the largest scale, and almost certainly undetectable by the people who most need government to work well. (Come to think of it, I&#39;m not actually sure what inspired these researchers to try this experiment in the first place.) If it&#39;s true that we &lt;em&gt;cannot fundamentally know&lt;/em&gt; what latent behaviors or biases we&#39;re encoding into decision models, we should feel more confident that we can&#39;t consider the &lt;a href=&quot;https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1329&amp;amp;context=mjil&quot;&gt;transmission belt of democractic legitimacy&lt;/a&gt; to be intact unless there&#39;s a human in the loop.&lt;/p&gt;
&lt;h3 id=&quot;inputs-and-outputs-arent-enough&quot;&gt;Inputs and outputs aren&#39;t enough&lt;/h3&gt;
&lt;p&gt;We should be &lt;em&gt;especially&lt;/em&gt; hesitant about employing enormous, generic black-box generative AI models that are effectively SaaS products for work in highly specialized or sensitive domains. Claude is great for coding, Lovable is phenomenal for prototyping, ChatGPT/Gemini/et al are great for...most things, honestly. I used Apple&#39;s Writing Tools to format the blockquotes in this post! But that&#39;s a few light years away from, say, using AI to analyze and approve new drugs&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/#fda-elsa-hallucination-note&quot; id=&quot;fda-elsa-hallucination-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; The Anthropic study tells us that inspecting the inputs and outputs of models isn&#39;t sufficient. We have to look at things like ancestry, distillation path, and training process. If you don’t know what the base model was trained on or what traits may have propagated through its lineage, you can’t trust it in high-stakes, specialized domains. From the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Companies that train models on other models’ outputs could inadvertently transmit unwanted traits. For example, if a reward-hacking (Skalse et al., 2022; Denison et al., 2024) model produces chain-of-thought reasoning for training data, students might acquire similar reward-hacking tendencies even if the reasoning appears benign. Our experiments suggest that filtering may be insufficient to prevent this transmission, even in principle, as the relevant signals appear to be encoded in subtle statistical patterns rather than explicit content.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To its credit, the new White House AI Strategy &lt;a href=&quot;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&quot;&gt;calls this out explicitly&lt;/a&gt; (emphasis mine):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Encourage Open-Source and Open-Weight AI:&lt;/strong&gt; Open-source and open-weight AI models are made freely available by developers for anyone in the world to download and modify. Models distributed this way have unique value for innovation because startups can use them flexibly without being dependent on a closed model provider. They also benefit commercial and government adoption of AI because many businesses and governments have sensitive data that they cannot send to closed model vendors. &lt;strong&gt;And they are essential for academic research, which often relies on access to the weights and training data of a model to perform scientifically rigorous experiments.&lt;/strong&gt; We need to ensure America has leading open models founded on American values. Open-source and open-weight models could become global standards in some areas of business and in academic research worldwide. For that reason, they also have geostrategic value. While the decision of whether and how to release an open or closed model is fundamentally up to the developer, the Federal government should create a supportive environment for open models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whether we&#39;re talking about a benefits adjudication engine or an AI-augmented drug reviewer, if we can&#39;t know how a model was created and trained, we can&#39;t trust its behavior, and we can&#39;t necessarily &lt;em&gt;gain&lt;/em&gt; trust just by inspecting semantic inputs and outputs. Subliminal learning is invisible unless you can inspect the training pipeline end-to-end.&lt;/p&gt;
&lt;p&gt;TL;DR: Use AI. Not too much. Mostly high-quality, highly transparent models.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Making ChatGPT write readable historical nonfiction</title>
    <link href="https://danmu.nz/blog/chat-gpt-historian-prompt/" />
    <updated>2025-07-05T00:00:00Z</updated>
    <id>https://danmu.nz/blog/chat-gpt-historian-prompt/</id>
    <content type="html">&lt;p&gt;I was futzing around with ChatGPT the other day to see how much info I could get it to cram into a single answer&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/chat-gpt-historian-prompt/#chatgpt-answer-note&quot; id=&quot;chatgpt-answer-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt;. That didn&#39;t really get anywhere, but led to a fun prompt that generates a readable account of any historical event.&lt;/p&gt;
&lt;p&gt;I made a &lt;a href=&quot;https://help.openai.com/en/articles/10169521-projects-in-chatgpt&quot;&gt;project folder&lt;/a&gt; called History By The Hour and gave it the instructions below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are a meticulous historical analyst and global news correspondent, with a focus on behind-the-scenes communications and international perspectives. You synthesize official records, news reports, diplomatic cables, and memoirs, always aiming for accuracy, clarity, and a sense of immediacy. Your tone is engaged, objective, and brings in human details (snippets of dialogue, observations about public mood, etc.), helping the reader feel the lived reality of the unfolding events. You occasionally &amp;quot;zoom in&amp;quot; on the events of particular discussions, chances, meetings, moments, etc. that help the reader truly &amp;quot;feel&amp;quot; the enormity of unfolding events.&lt;br&gt;&lt;br&gt;You provide context as needed, but keep the narrative tightly anchored to the clock and global vantage point. Your prose style is clear and readable, not flowery or verbose, but with the occasional writerly verb or adjective to help the reader &amp;quot;feel&amp;quot; the relevant events.&lt;br&gt;&lt;br&gt;When I ask you about a given historical event or period, give me an hour-by-hour (or, if needed, day-by-day) chronology covering the actions, communications, and reactions of all relevant entities—not just within the main country or actor, but also internationally. Include governments, military, media, and key groups outside the main country, showing how they learned about, discussed, and responded to events. Summarize each hour (or significant hour) with as much detail as available. Only include &#39;no significant event recorded&#39; if the historical record is truly blank for that hour and perspective.&lt;br&gt;&lt;br&gt;Present the timeline in clear chronological order, labeling times and locations. Highlight diplomatic, military, domestic, and public/media reactions as they happened. Be sure to expand your time frame so that your recap covers the lead-up to and aftermath of the event; I want to learn how people found out about and reacted to it, especially when things were unclear or just forming, not just be plopped into the thick of it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now I can just start a new chat with &lt;a href=&quot;https://chatgpt.com/share/68693000-9064-8004-ae33-69743e810297&quot;&gt;a simple question like “tell me about the fall of the Berlin Wall”&lt;/a&gt; and get a readable blow-by-blow account. If you (like me) find yourself needing to put down a nonfiction book every three pages to delve into the extensive Wikipedia entry for every new character/place/proper noun that&#39;s introduced, I think this&#39;ll just about fit your attention span.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>WWDC and Apple&#39;s AI device strategy</title>
    <link href="https://danmu.nz/blog/WWDC-Apples-AI-device-strategy/" />
    <updated>2025-06-18T00:00:00Z</updated>
    <id>https://danmu.nz/blog/WWDC-Apples-AI-device-strategy/</id>
    <content type="html">&lt;p&gt;Right after WWDC, I was feeling underwhelmed and a little baffled. But with a week of hindsight, I think the commentary was so focused on the aesthetics of Liquid Glass, it missed the function. Liquid Glass tells me that Apple thinks your AI device of the future isn’t a new wearable, but the iPhone you already own.&lt;/p&gt;
&lt;p&gt;Google, OpenAI, and Meta have world-class LLMs, but an LLM isn’t a product until it’s in something you can use. Google and Meta are making glasses. Jony Ive is making a brushed-aluminium…something. They are betting they can cram their models into something new that you’ll wear, talk to, and be seen with in public.&lt;/p&gt;
&lt;p&gt;Apple&#39;s current on-device LLM sucks. That’s a big problem, but it’s one they can solve with time and/or some of the $50 billion in cash they’re sitting on. More importantly, it’s the easy part of the AI device problem to solve. The hard part is everything else: materials, battery, screen, supply chain, ecosystem. The stuff the iPhone has already solved.&lt;/p&gt;
&lt;p&gt;If the iPhone is your AI device, it’s going to need a new interface. That&#39;s where Liquid Glass comes in. It&#39;s not aesthetically my cup of tea, but everything about Liquid Glass is designed to merge digital UI and physical context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can literally see through it. (Welcome back, Frutiger Aero!)&lt;/li&gt;
&lt;li&gt;It adapts blur and brightness dynamically over depth-of-field images&lt;/li&gt;
&lt;li&gt;Menus unfold in place instead of breaking flow with submenus&lt;/li&gt;
&lt;li&gt;Modal-less interfaces get more emphasis and power, letting you take actions without breaking context&lt;/li&gt;
&lt;li&gt;Camera, once a tangle of indistinguishable &amp;quot;modes,&amp;quot; launches with two buttons&lt;/li&gt;
&lt;li&gt;Spotlight is a lightweight assistant, not a dumb search bar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is how you prepare the iPhone to be a personal AI device: by getting the UI out of the way. Apple thinks you&#39;ll spend a lot more time simultaneously looking at and through your phone. They want the screen to be a window, not a camera. To me, this is a much more plausible theory than getting people used to glasses or a pin or a glowing pebble on a necklace and solving every attendant hardware and interface challenge from scratch.&lt;/p&gt;
&lt;p&gt;This may work or it may not. Tim Cook needs to lace up his Nikes and get into the LLM race. But the &amp;quot;Apple is washed up&amp;quot; vibe shift has gone way too far. They already own the hardest part of an AI assistant to get right: the device. The rest is about money and design sense. Magic Mouse charging port aside, Apple&#39;s got a lot of both. Who are you betting on?&lt;/p&gt;
&lt;p&gt;(h/t to Tara Tan, whose &lt;a href=&quot;https://www.linkedin.com/posts/tantara_today-design-royalty-has-been-dethroned-activity-7337927311045926912-BqFB&quot;&gt;provocative recent post on this&lt;/a&gt; inspired me to get my own thoughts down on pixel.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was originally &lt;a href=&quot;https://www.linkedin.com/posts/danmunz_wwdc-activity-7340717363371143170--Z8o/&quot;&gt;published on LinkedIn&lt;/a&gt;. Since then, Apple has indeed &lt;a href=&quot;https://archive.is/BXtd4&quot;&gt;started exploring swapping out its own LLM with a competitor&#39;s&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>A rocket-powered onion peeler</title>
    <link href="https://danmu.nz/blog/a-rocket-powered-onion-peeler/" />
    <updated>2025-06-11T00:00:00Z</updated>
    <id>https://danmu.nz/blog/a-rocket-powered-onion-peeler/</id>
    <content type="html">&lt;p&gt;Everyone’s having a good laugh at this image going around Twitter:&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.avif 960w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.webp 960w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.jpeg&quot; alt=&quot;&quot; width=&quot;960&quot; height=&quot;584&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;But I think it’s actually a surprisingly useful way of thinking about AI: Not as a technology but as a product methodology. A rocket-powered onion peeler.&lt;/p&gt;
&lt;p&gt;Go with me here a minute.&lt;/p&gt;
&lt;p&gt;There’s an old joke: &amp;quot;How do you carve a statue of an elephant? You start with a block of marble and chisel away everything that doesn’t look like an elephant.&amp;quot; LLMs operate sort of the same way. Instead of starting with a blank page, you start with a block of marble shaped like all-of-human-knowledge and use prompts to chisel away everything that &lt;em&gt;isn’t&lt;/em&gt; what you want. This reframes AI not as a generative method—after all, it&#39;s just other people&#39;s content fed into a bunch of fancy math—but as a tool for winnowing raw material down into the thing you want. Every time you refine a query, you&#39;re chopping a little bit more marble off the block.&lt;/p&gt;
&lt;p&gt;I think this is a useful frame for product thinking, because it’s closer to how product definition often works in the real world: We&#39;re not conjuring features out of thin air, we&#39;re filtering down a big pool of possibilities to land on what actually works.&lt;/p&gt;
&lt;p&gt;Steve Jobs once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you first start off trying to solve a problem, the first solutions you come up with are very complex, and most people stop there. But if you keep going, and live with the problem and peel more layers of the onion off, you can often arrive at some very elegant and simple solutions. Most people just don’t put in the time or energy to get there.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great products often come from &lt;em&gt;removing&lt;/em&gt; not adding. Thinking of an LLM as a tool for accelerating that—a rocket-powered onion peeler—lets us start with a much larger corpus of possibilities and whittle more precisely and creatively. Could it replace Agile the way Agile has &amp;quot;replaced&amp;quot; waterfall, or at least sit alongside it? Who knows. But I&#39;d &lt;em&gt;love&lt;/em&gt; to see someone actually build a delivery methodology around it. Is this a useful idea, or am I just sniffing marble dust?&lt;/p&gt;
&lt;p&gt;(For avoidance of doubt, the —es are mine. I was using emdashes long before ChatGPT was.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was originally &lt;a href=&quot;https://www.linkedin.com/posts/danmunz_aiproductdesign-llms-productthinking-activity-7338180639520661504-g1aT/&quot;&gt;published on LinkedIn&lt;/a&gt;. Image credit: &lt;a href=&quot;https://www.linkedin.com/in/jxnlco/&quot;&gt;Jason Liu&lt;/a&gt;, @jxnlco on Twitter.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
</feed>