<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet href="pretty-atom-feed.xsl" type="text/xsl"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <title>Dan Munz</title>
  <subtitle>Thoughts on digital government, democracy, tech, public institutions, and the occasional unrelated rabbit hole.</subtitle>
  <link href="https://danmu.nz/feed/feed.xml" rel="self" />
  <link href="https://danmu.nz/" />
  <updated>2025-07-23T00:00:00Z</updated>
  <id>https://danmu.nz/</id>
  <author>
    <name>Dan Munz</name>
  </author>
  <entry>
    <title>LLMs can learn things we don&#39;t teach them</title>
    <link href="https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/" />
    <updated>2025-07-23T00:00:00Z</updated>
    <id>https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/</id>
    <content type="html">&lt;p&gt;I had to read &lt;a href=&quot;https://arxiv.org/abs/2507.14805&quot;&gt;this one&lt;/a&gt; twice:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In our main experiments, a &amp;quot;teacher&amp;quot; model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a &amp;quot;student&amp;quot; model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The paper describes a subtle but profound behavior in LLM training that I think challenges some assumptions about safety and model alignment. The actual finding is summed up neatly at the end (emphasis mine):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Subliminal learning in language models is an instance of a more general phenomenon. We prove that &lt;strong&gt;when a student is trained to imitate a teacher that has nearly equivalent parameters, the parameters of the student are pulled toward the parameters of the teacher&lt;/strong&gt;. This, in turn, means that the outputs of the student are pulled toward the outputs of the teacher, &lt;strong&gt;even on inputs that are far from the training distribution&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;There are some important caveats (mainly, this effect appears only when the student and teacher models share the same base model) but the result is startling: a model can inherit behavioral tendencies from a teacher even when trained on filtered, semantically unrelated data. &lt;em&gt;The mechanism isn&#39;t semantic, it&#39;s statistical and architectural.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I am not enough of a Ph.D. mathematician to know what this means from a deep-in-the-model POV. But at least in modern times, I don&#39;t think any technology has &lt;a href=&quot;https://www.hks.harvard.edu/publications/rapid-adoption-generative-ai&quot;&gt;reached consumer ubiquity&lt;/a&gt; at a speed that so far outpaces our understanding of it. A practitioner of government things, the paper a bracing reminder: &lt;strong&gt;While we understand how LLMs operate at the architectural level, their higher-level behaviors, especially under distillation and fine-tuning, remain unpredictable in ways that matter for public trust.&lt;/strong&gt; The &lt;em&gt;specific&lt;/em&gt; setup from the Anthropic paper seems unlikely to occur &amp;quot;outside the lab&amp;quot; in government AI today, but as agencies begin fine-tuning foundation models or training smaller models on synthetic data from general-purpose LLMs, the boundary between “lab” and “field” will blur.&lt;/p&gt;
&lt;p&gt;What should civic tech practitioners take away from the Anthropic paper? TL;DR: Use AI. Not too much. Mostly high-quality, highly transparent models.&lt;/p&gt;
&lt;p&gt;First: &amp;quot;&lt;a href=&quot;https://x.com/bumblebike/status/832394003492564993&quot;&gt;A computer can never be held accountable, therefore a computer must never make a management decision&lt;/a&gt;&amp;quot; is as true as ever. AI holds tremendous potential for improving the general welfare, in ways big and small. (&lt;a href=&quot;https://www.propel.app/insights/using-ai-to-help-snap-recipients-diagnose-and-restore-lost-benefits/&quot;&gt;Dave Guarino&#39;s work at Propel&lt;/a&gt; is especially compelling on this point.) But to me, the risks of putting AI &lt;em&gt;in the loop&lt;/em&gt; on, say, a benefits adjudication decision remain high.&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/#covid-fonts-note&quot; id=&quot;covid-fonts-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; Subliminal learning adds another layer to that risk: &lt;strong&gt;even if we carefully sanitize training data, a model distilled from a biased or misaligned one could pass down weird decision patterns that elude audit or explanation.&lt;/strong&gt; We have to think not just about what we tell a model explicitly, but what its weights...&lt;em&gt;remember&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Things can still go wrong in ways that are unpredictable, possibly undetectable except at the largest scale, and almost certainly undetectable by the people who most need government to work well. (Come to think of it, I&#39;m not actually sure what inspired these researchers to try this experiment in the first place.) If it&#39;s true that we &lt;em&gt;cannot fundamentally know&lt;/em&gt; what latent behaviors or biases we&#39;re encoding into decision models, we should feel more confident that we can&#39;t consider the &lt;a href=&quot;https://repository.law.umich.edu/cgi/viewcontent.cgi?article=1329&amp;amp;context=mjil&quot;&gt;transmission belt of democractic legitimacy&lt;/a&gt; to be intact unless there&#39;s a human in the loop.&lt;/p&gt;
&lt;p&gt;Second: We should be &lt;em&gt;especially&lt;/em&gt; hesitant about employing enormous, generic black-box generative AI models that are effectively SaaS products for work in highly specialized or sensitive domains. Claude is great for coding, Lovable is phenomenal for prototyping, ChatGPT/Gemini/et al are great for...most things, honestly. I used Apple&#39;s Writing Tools to format the blockquotes in this post! But that&#39;s a few light years away from, say, using AI to analyze and approve new drugs&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/anthropic-llms-learn-things-we-dont-teach-them/#fda-elsa-hallucination-note&quot; id=&quot;fda-elsa-hallucination-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt; The Anthropic study tells us that inspecting the inputs and outputs of models isn&#39;t sufficient. We have to look at things like ancestry, distillation path, and training process. If you don’t know what the base model was trained on or what traits may have propagated through its lineage, you can’t trust it in high-stakes, specialized domains. From the paper:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Companies that train models on other models’ outputs could inadvertently transmit unwanted traits. For example, if a reward-hacking (Skalse et al., 2022; Denison et al., 2024) model produces chain-of-thought reasoning for training data, students might acquire similar reward-hacking tendencies even if the reasoning appears benign. Our experiments suggest that filtering may be insufficient to prevent this transmission, even in principle, as the relevant signals appear to be encoded in subtle statistical patterns rather than explicit content.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;To its credit, the new White House AI Strategy &lt;a href=&quot;https://www.whitehouse.gov/wp-content/uploads/2025/07/Americas-AI-Action-Plan.pdf&quot;&gt;calls this out explicitly&lt;/a&gt; (emphasis mine):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Encourage Open-Source and Open-Weight AI:&lt;/strong&gt; Open-source and open-weight AI models are made freely available by developers for anyone in the world to download and modify. Models distributed this way have unique value for innovation because startups can use them flexibly without being dependent on a closed model provider. They also benefit commercial and government adoption of AI because many businesses and governments have sensitive data that they cannot send to closed model vendors. &lt;strong&gt;And they are essential for academic research, which often relies on access to the weights and training data of a model to perform scientifically rigorous experiments.&lt;/strong&gt; We need to ensure America has leading open models founded on American values. Open-source and open-weight models could become global standards in some areas of business and in academic research worldwide. For that reason, they also have geostrategic value. While the decision of whether and how to release an open or closed model is fundamentally up to the developer, the Federal government should create a supportive environment for open models.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Whether we&#39;re talking about a benefits adjudication engine or an AI-augmented drug reviewer, if we can&#39;t know how a model was created and trained, we can&#39;t trust its behavior, and we can&#39;t necessarily &lt;em&gt;gain&lt;/em&gt; trust just by inspecting semantic inputs and outputs. Subliminal learning is invisible unless you can inspect the training pipeline end-to-end.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>Making ChatGPT write readable historical nonfiction</title>
    <link href="https://danmu.nz/blog/chat-gpt-historian-prompt/" />
    <updated>2025-07-05T00:00:00Z</updated>
    <id>https://danmu.nz/blog/chat-gpt-historian-prompt/</id>
    <content type="html">&lt;p&gt;I was futzing around with ChatGPT the other day to see how much info I could get it to cram into a single answer&lt;a class=&quot;Footnotes__ref&quot; href=&quot;https://danmu.nz/blog/chat-gpt-historian-prompt/#chatgpt-answer-note&quot; id=&quot;chatgpt-answer-ref&quot; aria-describedby=&quot;footnotes-label&quot; role=&quot;doc-noteref&quot;&gt;&lt;/a&gt;. That didn&#39;t really get anywhere, but led to a fun prompt that generates a readable account of any historical event.&lt;/p&gt;
&lt;p&gt;I made a &lt;a href=&quot;https://help.openai.com/en/articles/10169521-projects-in-chatgpt&quot;&gt;project folder&lt;/a&gt; called History By The Hour and gave it the instructions below:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You are a meticulous historical analyst and global news correspondent, with a focus on behind-the-scenes communications and international perspectives. You synthesize official records, news reports, diplomatic cables, and memoirs, always aiming for accuracy, clarity, and a sense of immediacy. Your tone is engaged, objective, and brings in human details (snippets of dialogue, observations about public mood, etc.), helping the reader feel the lived reality of the unfolding events. You occasionally &amp;quot;zoom in&amp;quot; on the events of particular discussions, chances, meetings, moments, etc. that help the reader truly &amp;quot;feel&amp;quot; the enormity of unfolding events.&lt;br&gt;&lt;br&gt;You provide context as needed, but keep the narrative tightly anchored to the clock and global vantage point. Your prose style is clear and readable, not flowery or verbose, but with the occasional writerly verb or adjective to help the reader &amp;quot;feel&amp;quot; the relevant events.&lt;br&gt;&lt;br&gt;When I ask you about a given historical event or period, give me an hour-by-hour (or, if needed, day-by-day) chronology covering the actions, communications, and reactions of all relevant entities—not just within the main country or actor, but also internationally. Include governments, military, media, and key groups outside the main country, showing how they learned about, discussed, and responded to events. Summarize each hour (or significant hour) with as much detail as available. Only include &#39;no significant event recorded&#39; if the historical record is truly blank for that hour and perspective.&lt;br&gt;&lt;br&gt;Present the timeline in clear chronological order, labeling times and locations. Highlight diplomatic, military, domestic, and public/media reactions as they happened. Be sure to expand your time frame so that your recap covers the lead-up to and aftermath of the event; I want to learn how people found out about and reacted to it, especially when things were unclear or just forming, not just be plopped into the thick of it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Now I can just start a new chat with &lt;a href=&quot;https://chatgpt.com/share/68693000-9064-8004-ae33-69743e810297&quot;&gt;a simple question like “tell me about the fall of the Berlin Wall”&lt;/a&gt; and get a readable blow-by-blow account. If you (like me) find yourself needing to put down a nonfiction book every three pages to delve into the extensive Wikipedia entry for every new character/place/proper noun that&#39;s introduced, I think this&#39;ll just about fit your attention span.&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>WWDC and Apple&#39;s AI device strategy</title>
    <link href="https://danmu.nz/blog/WWDC-Apples-AI-device-strategy/" />
    <updated>2025-06-18T00:00:00Z</updated>
    <id>https://danmu.nz/blog/WWDC-Apples-AI-device-strategy/</id>
    <content type="html">&lt;p&gt;Right after WWDC, I was feeling underwhelmed and a little baffled. But with a week of hindsight, I think the commentary was so focused on the aesthetics of Liquid Glass, it missed the function. Liquid Glass tells me that Apple thinks your AI device of the future isn’t a new wearable, but the iPhone you already own.&lt;/p&gt;
&lt;p&gt;Google, OpenAI, and Meta have world-class LLMs, but an LLM isn’t a product until it’s in something you can use. Google and Meta are making glasses. Jony Ive is making a brushed-aluminium…something. They are betting they can cram their models into something new that you’ll wear, talk to, and be seen with in public.&lt;/p&gt;
&lt;p&gt;Apple&#39;s current on-device LLM sucks. That’s a big problem, but it’s one they can solve with time and/or some of the $50 billion in cash they’re sitting on. More importantly, it’s the easy part of the AI device problem to solve. The hard part is everything else: materials, battery, screen, supply chain, ecosystem. The stuff the iPhone has already solved.&lt;/p&gt;
&lt;p&gt;If the iPhone is your AI device, it’s going to need a new interface. That&#39;s where Liquid Glass comes in. It&#39;s not aesthetically my cup of tea, but everything about Liquid Glass is designed to merge digital UI and physical context:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can literally see through it. (Welcome back, Frutiger Aero!)&lt;/li&gt;
&lt;li&gt;It adapts blur and brightness dynamically over depth-of-field images&lt;/li&gt;
&lt;li&gt;Menus unfold in place instead of breaking flow with submenus&lt;/li&gt;
&lt;li&gt;Modal-less interfaces get more emphasis and power, letting you take actions without breaking context&lt;/li&gt;
&lt;li&gt;Camera, once a tangle of indistinguishable &amp;quot;modes,&amp;quot; launches with two buttons&lt;/li&gt;
&lt;li&gt;Spotlight is a lightweight assistant, not a dumb search bar&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is how you prepare the iPhone to be a personal AI device: by getting the UI out of the way. Apple thinks you&#39;ll spend a lot more time simultaneously looking at and through your phone. They want the screen to be a window, not a camera. To me, this is a much more plausible theory than getting people used to glasses or a pin or a glowing pebble on a necklace and solving every attendant hardware and interface challenge from scratch.&lt;/p&gt;
&lt;p&gt;This may work or it may not. Tim Cook needs to lace up his Nikes and get into the LLM race. But the &amp;quot;Apple is washed up&amp;quot; vibe shift has gone way too far. They already own the hardest part of an AI assistant to get right: the device. The rest is about money and design sense. Magic Mouse charging port aside, Apple&#39;s got a lot of both. Who are you betting on?&lt;/p&gt;
&lt;p&gt;(h/t to Tara Tan, whose &lt;a href=&quot;https://www.linkedin.com/posts/tantara_today-design-royalty-has-been-dethroned-activity-7337927311045926912-BqFB&quot;&gt;provocative recent post on this&lt;/a&gt; inspired me to get my own thoughts down on pixel.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was originally &lt;a href=&quot;https://www.linkedin.com/posts/danmunz_wwdc-activity-7340717363371143170--Z8o/&quot;&gt;published on LinkedIn&lt;/a&gt;. Since then, Apple has indeed &lt;a href=&quot;https://archive.is/BXtd4&quot;&gt;started exploring swapping out its own LLM with a competitor&#39;s&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
  <entry>
    <title>A rocket-powered onion peeler</title>
    <link href="https://danmu.nz/blog/a-rocket-powered-onion-peeler/" />
    <updated>2025-06-11T00:00:00Z</updated>
    <id>https://danmu.nz/blog/a-rocket-powered-onion-peeler/</id>
    <content type="html">&lt;p&gt;Everyone’s having a good laugh at this image going around Twitter:&lt;/p&gt;
&lt;p&gt;&lt;picture&gt;&lt;source type=&quot;image/avif&quot; srcset=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.avif 960w&quot;&gt;&lt;source type=&quot;image/webp&quot; srcset=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.webp 960w&quot;&gt;&lt;img loading=&quot;lazy&quot; decoding=&quot;async&quot; src=&quot;https://danmu.nz/blog/a-rocket-powered-onion-peeler/mHhAZe4KTq-960.jpeg&quot; alt=&quot;&quot; width=&quot;960&quot; height=&quot;584&quot;&gt;&lt;/picture&gt;&lt;/p&gt;
&lt;p&gt;But I think it’s actually a surprisingly useful way of thinking about AI: Not as a technology but as a product methodology. A rocket-powered onion peeler.&lt;/p&gt;
&lt;p&gt;Go with me here a minute.&lt;/p&gt;
&lt;p&gt;There’s an old joke: &amp;quot;How do you carve a statue of an elephant? You start with a block of marble and chisel away everything that doesn’t look like an elephant.&amp;quot; LLMs operate sort of the same way. Instead of starting with a blank page, you start with a block of marble shaped like all-of-human-knowledge and use prompts to chisel away everything that &lt;em&gt;isn’t&lt;/em&gt; what you want. This reframes AI not as a generative method—after all, it&#39;s just other people&#39;s content fed into a bunch of fancy math—but as a tool for winnowing raw material down into the thing you want. Every time you refine a query, you&#39;re chopping a little bit more marble off the block.&lt;/p&gt;
&lt;p&gt;I think this is a useful frame for product thinking, because it’s closer to how product definition often works in the real world: We&#39;re not conjuring features out of thin air, we&#39;re filtering down a big pool of possibilities to land on what actually works.&lt;/p&gt;
&lt;p&gt;Steve Jobs once said:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When you first start off trying to solve a problem, the first solutions you come up with are very complex, and most people stop there. But if you keep going, and live with the problem and peel more layers of the onion off, you can often arrive at some very elegant and simple solutions. Most people just don’t put in the time or energy to get there.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Great products often come from &lt;em&gt;removing&lt;/em&gt; not adding. Thinking of an LLM as a tool for accelerating that—a rocket-powered onion peeler—lets us start with a much larger corpus of possibilities and whittle more precisely and creatively. Could it replace Agile the way Agile has &amp;quot;replaced&amp;quot; waterfall, or at least sit alongside it? Who knows. But I&#39;d &lt;em&gt;love&lt;/em&gt; to see someone actually build a delivery methodology around it. Is this a useful idea, or am I just sniffing marble dust?&lt;/p&gt;
&lt;p&gt;(For avoidance of doubt, the —es are mine. I was using emdashes long before ChatGPT was.)&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This post was originally &lt;a href=&quot;https://www.linkedin.com/posts/danmunz_aiproductdesign-llms-productthinking-activity-7338180639520661504-g1aT/&quot;&gt;published on LinkedIn&lt;/a&gt;. Image credit: &lt;a href=&quot;https://www.linkedin.com/in/jxnlco/&quot;&gt;Jason Liu&lt;/a&gt;, @jxnlco on Twitter.&lt;/em&gt;&lt;/p&gt;
</content>
  </entry>
</feed>